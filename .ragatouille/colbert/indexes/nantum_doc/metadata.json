{
  "config":{
    "query_token_id":"[unused0]",
    "doc_token_id":"[unused1]",
    "query_token":"[Q]",
    "doc_token":"[D]",
    "ncells":null,
    "centroid_score_threshold":null,
    "ndocs":null,
    "load_index_with_mmap":false,
    "index_path":null,
    "index_bsize":32,
    "nbits":8,
    "kmeans_niters":20,
    "resume":false,
    "similarity":"cosine",
    "bsize":64,
    "accumsteps":1,
    "lr":0.00001,
    "maxsteps":400000,
    "save_every":null,
    "warmup":20000,
    "warmup_bert":null,
    "relu":false,
    "nway":64,
    "use_ib_negatives":true,
    "reranker":false,
    "distillation_alpha":1.0,
    "ignore_scores":false,
    "model_name":null,
    "query_maxlen":32,
    "attend_to_mask_tokens":false,
    "interaction":"colbert",
    "dim":128,
    "doc_maxlen":512,
    "mask_punctuation":true,
    "checkpoint":"colbert-ir/colbertv2.0",
    "triples":"/future/u/okhattab/root/unit/experiments/2021.10/downstream.distillation.round2.2_score/round2.nway6.cosine.ib/examples.64.json",
    "collection":[
      "PD Research Documentation\n \n version\n \n 2021, Prescriptive Data, Inc.\n \n June 18, 2024Contents\nPD Research\n 1\n NANTUM Energy Conservation Measures (ECMs)\n 1\n NANTUM ECM Development Cycle\n 1\n Phase 0: Concept and Research\n 1\n Phase 1: ECM Design and Experimental Test\n 1\n Phase 2: Package and Test Environment\n 1\n Phase 3: Production and Maintenance\n 1\n ECM#1 and #3: Recommended and Automated Startup\n 2\n Overview\n 2\n Algorithm\n 2\n Required Data\n 2\n Required BMS Programming\n 2\n Link to Full Documentation\n 2\n ECM#2: Automated Ramps\n 2\n Introduction\n 2\n Usability/ Limitations\n 2\n Installation and run\n 2\n Modules\n 3\n extremum prediction\n 3\n floor level\n 3\n n ramps\n 3\n utils\n 3\n main\n 3\n Requirement\n 4\n Examples\n 7\n Errors and Solutions\n 7\n ECM#4: Peak Demand Prediction\n 7\n Overview\n 7\n Algorithm\n 7\n Required Data\n 7\n Required BMS Programming\n 7\n ECM#5: Ramp-Based Peak Management (Game of Fans)\n 7\n Overview\n 7\n Demand-Side Management (DSM)\n 7\n Expanding on Nantum Ramps\n 8\n Algorithm\n 8\n Space Temperature model\n 8\n Optimization Formulation\n 9\n Demand Response (Reduction) Orders\n 11\n Required Data\n 11\n Required BMS Programming\n 11\n Link to Full Documentation\n 11\n ECM#6: Chilled Water Pump Staging\n 11\n Overview\n 11Algorithm\n 12\n Required Data\n 12\n Required BMS Programming\n 12\n Link to Full Documentation\n 12\n ECM#7: Condenser Water Pump Staging\n 12\n Overview\n 12\n Algorithm\n 13\n Required Data\n 13\n Required BMS Programming\n 13\n ECM#8: Condenser Water Flow Optimization\n 13\n Overview\n 13\n Algorithm\n 13\n Required Data\n 13\n Required BMS Programming\n 13\n ECM#9: Condenser Water Supply Temperature Optimization\n 13\n Overview\n 13\n Algorithm\n 13\n Required Data",
      "ECM#8: Condenser Water Flow Optimization\n 13\n Overview\n 13\n Algorithm\n 13\n Required Data\n 13\n Required BMS Programming\n 13\n ECM#9: Condenser Water Supply Temperature Optimization\n 13\n Overview\n 13\n Algorithm\n 13\n Required Data\n 13\n Required BMS Programming\n 13\n ECM#14: Static Pressure Optimization\n 13\n Overview\n 13\n Algorithm\n 13\n Using AHU base model\n 13\n Using AHU high-resolution model\n 14\n Required Data\n 15\n Minimum Requirements\n 15\n Additional Requirements\n 15\n Required BMS Programming\n 15\n Link to Full Documentation\n 15\n ECM#16: Chilled Water Flow Optimization\n 16\n Overview\n 16\n Algorithm\n 16\n Required Data\n 16\n Required BMS Programming\n 16\n Link to Full Documentation\n 16\n ECM#17: NANTUM Automated Demand Management (ADM)\n 16\n Overview\n 16\n Algorithm\n 18\n Overview\n 18\n Required Input for the ECM\n 19\n Short and Long Term Demand Prediction\n 19\n Generating Optimal Demand Limit\n 19\n Monitoring Demand Limit\n 19\n Required Data\n 20\n Required BMS Programming\n 20\n Link to Full Documentation\n 20ECM#18: Interior Space Temperature Optimization\n 20\n Overview\n 20\n Algorithm\n 20\n Stage 0: Occupancy-Driven Temperature Optimization\n 20\n Stage 1: Predictive Control of Temperature\n 21\n Stage 2: Demand-Side Management(DSM) with Temperature Control\n 21\n Required Data\n 21\n Required BMS Programming\n 21\n Link to Full Documentation\n 21\n ECM#19: Building Preheat Optmization\n 21\n Overview\n 21\n Potential Savings\n 21\n Algorithm\n 22\n Link to Full Documentation\n 22\n Benchmarks\n 22\n Energy Benchmark\n 22\n Introduction\n 22\n System diagram\n 22\n Usability/ Limitations\n 23\n Installation and run\n 23\n Modules\n 23\n data_handler\n 23\n logic\n 24\n main\n 24\n Requirement\n 24\n Examples\n 25\n Errors and Solutions\n 26\n Water Benchmark\n 26\n Introduction\n 26\n Usability/ Limitations\n 26\n Installation and run",
      "Modules\n 23\n data_handler\n 23\n logic\n 24\n main\n 24\n Requirement\n 24\n Examples\n 25\n Errors and Solutions\n 26\n Water Benchmark\n 26\n Introduction\n 26\n Usability/ Limitations\n 26\n Installation and run\n 26\n Modules\n 27\n data_handler\n 27\n train\n 27\n utils\n 27\n main\n 27\n Requirement\n 27\n Examples\n 28\n Errors and Solutions\n 28\n Occupancy\n 28\n Wi-Fi Occupancy\n 28\n Overview\n 28\n Link to Full Documentation\n 29\n Human Comfort\n 30\n Comfort Index\n 30\n Introduction\n 30Installation and run\n 30\n Modules\n 30\n Data handler\n 30\n logic\n 30\n main\n 31\n Requirement\n 31\n Examples\n 31\n Errors and Solutions\n 31\n Space Utilization\n 31\n Using a Mobile Robot\n 31\n Overview\n 31\n Robot Selection\n 31\n Procedures\n 31\n Initial Results\n 32\n Floor Mapping with SLAM\n 32\n Locational Sampling\n 33\n RSSI Modeling\n 33\n Space Utilization with RSSI model\n 35\n Monitoring and Measurement\n 37\n NANTUM Potential Saving Calculator\n 37\n Energy\n 40\n Chiller Retrofit - Steam Turbine to Electric\n 40\n Overview\n 40\n Efficiency of Steam and Electric Chiller\n 40\n Cost of Steam and Electric\n 41\n Example: Switching to Electric (345 Park Avenue, 2019)\n 43\n How Should Buildings Operate with A Mixed of Both chillers\n 44\n Greenhouse Gas (GHG)\n 45\n Local Law 97\n 45\n NANTUM GHG Emissions App\n 45\n Overview\n 45\n NANTUM UI\n 45\n Average GHG Multiplier\n 48\n Real-Time GHG Multiplier\n 48\n NYISO Real-Time Data\n 48\n WattTime\n 50\n Carbonara, Singularity Energy\n 51\n Measuring Building GHG Using Real-Time Multiplier\n 51\n Anomaly Detection\n 54\n Anomaly Detection\n 54\n Feature Anomaly Detection\n 54\n Time-Series Predictive Anomaly Detection\n 55\n The Algorithm Evaluates:\n 55\n Supported Resources:\n 55\n Multivariate",
      "Measuring Building GHG Using Real-Time Multiplier\n 51\n Anomaly Detection\n 54\n Anomaly Detection\n 54\n Feature Anomaly Detection\n 54\n Time-Series Predictive Anomaly Detection\n 55\n The Algorithm Evaluates:\n 55\n Supported Resources:\n 55\n Multivariate Anomaly Detection\n 56\n NANTUM Common Modules\n 56Day Type Prediction\n 56\n Introduction\n 56\n Usability/ Limitations\n 56\n Installation and run\n 56\n Modules\n 57\n data_handler\n 57\n same_class_days\n 57\n Requirement\n 57\n Examples\n 57\n Errors and Solutions\n 58\n Billing Engine\n 58\n Introduction\n 58\n System Diagram\n 58\n Installation\n 58\n Modules\n 58\n Data handler\n 58\n Rules handler\n 58\n Rates Logic\n 58\n Statement Builder\n 58\n Examples\n 59\n Errors and Solutions\n 59\n Similar Days\n 59\n Introduction\n 59\n Usability/ Limitations\n 59\n Installation and run\n 59\n Modules\n 60\n data_handler\n 60\n main\n 60\n utils\n 61\n Requirement\n 61\n Examples\n 61\n Errors and Solutions\n 61\n Training Pipeline\n 61\n Introduction\n 61\n Installation and run\n 61\n Modules\n 62\n data_processor\n 62\n callbacks\n 62\n model\n 62\n evaluate\n 62\n main\n 62\n Examples\n 63\n Errors and Solutions\n 63\n TTRCL\n 63\n Introduction\n 63\n Usability/ Limitations\n 63Installation and run\n 64\n Modules\n 64\n data_handler\n 64\n days_classification\n 64\n Similar_days\n 64\n logic\n 65\n train\n 65\n predict\n 65\n sla_temperature\n 65\n unregulated_temperature\n 65\n utils\n 65\n main\n 65\n Requirement\n 65\n Examples\n 65\n Errors and Solutions\n 65\n Demand Forecast\n 65\n Introduction\n 65\n Usability/ Limitations\n 66\n Installation and run\n 66\n Modules\n 66\n data_handler\n 66\n train\n 67\n utils\n 67\n main\n 67\n Requirement\n 67\n Examples\n 68\n Errors and Solutions\n 69\n Peak Demand\n 69",
      "Limitations\n 66\n Installation and run\n 66\n Modules\n 66\n data_handler\n 66\n train\n 67\n utils\n 67\n main\n 67\n Requirement\n 67\n Examples\n 68\n Errors and Solutions\n 69\n Peak Demand\n 69\n Introduction\n 69\n Usability/ Limitations\n 69\n Installation and run\n 69\n Modules\n 69\n generate_peak\n 69\n utils\n 69\n main\n 70\n Requirement\n 70\n Examples\n 70\n Errors and Solutions\n 70\n Predictive Demand\n 70\n Introduction\n 70\n System diagram\n 71\n Usability/ Limitations\n 71\n Installation and run\n 71\n Modules\n 72\n data_handler\n 72\n consumption_trainer\n 72train\n 72\n predict\n 72\n day_attributes\n 72\n main\n 72\n Requirements\n 73\n Examples\n 74\n Errors and Solutions\n 74\n Onboarding\n 74\n Analytics Onboard Requirement\n 74\n ECM Input/Output Standard\n 74\n Automated Recognition and Mapping (ARM)\n 75\n Overview\n 75\n Algorithm\n 75\n Unsupervised Engine\n 75\n Implementation and Results\n 75\n Limitations and Future Works\n 75\n NANTUM Marketplace\n 75\n AI of AIs (Nanum Decentralized AI Architecture)\n 75\n External Energy Conservation Measures (ECMs)\n 76\n Brain4Energy ECM\n 76\n Phase 1: Air-Side Optimization\n 76\n Summary\n 76\n Loop Diagram\n 77\n Required Input\n 78\n Potential Output\n 78\n Implementation Requirements\n 79\n Case Study\n 79PD Research\nNANTUM Energy Conservation Measures (ECMs)\nNANTUM ECM Development Cycle\nPhase 0: Concept and Research\nConcepting of ECM could come from\n1.\nIdeas.\n2.\nSpecific needs.\n3.\nRequests.\nResearching for the ECM should include\n1.\nKnowledge field research.\n2.\nImpact and potential saving research.\n3.\nImplementation feasibility research\nPhase 0 Milestones\n\nFinished potential impact research.\n\nFinished potential saving estimation.\n\nListed required data and information.\nPhase 1: ECM Design and Experimental Test\nDesigning and testing should be happening simultaneously and iteratively during the initial stages of the\ndevelopment of ECM.",
      "\nFinished potential saving estimation.\n\nListed required data and information.\nPhase 1: ECM Design and Experimental Test\nDesigning and testing should be happening simultaneously and iteratively during the initial stages of the\ndevelopment of ECM.\nPhase 1 Milestones\n\nPerformed necessary tests.\n\nFinalized ECM algorithm design.\n\nTested with example inputs.\nPhase 2: Package and Test Environment\nPhase 2 moves ECM development from offline to online by finalizing the algorithm side of the ECM and adding the\nnecessary connectivity to database, BMS, and Nantum. The ECM should also be tested in the Core Research test\nenvironment in preparation for production.\nPhase 2 Milestones\n\nDelivered finalized ECM algorithm functions.\n\nRun ECM successfully in test environment.\nPhase 3: Production and Maintenance\nAn active ECM should be maintained in phase 3 where the following four possible types of work should be continued\n1.\nProduction for initial buildings.\n2.\nMaintenance for current buildings.\n3.\nImprovement on algorithm.\n4.\nModification and incorporation for different buildings.\nPD Research\n1Phase 3 Milestone\n\nRun ECM successfully fully automated in at least one building.\nECM#1 and #3: Recommended and Automated Startup\nOverview\nAlgorithm\nRequired Data\nRequired BMS Programming\nLink to Full Documentation\nECM_Startup_Shutdown\nECM#2: Automated Ramps\nIntroduction\nThis system generates the ramp timings for the building taking into account occupancy data. The ramp timings are\ngenerated at maxima(ramp down) and minima(ramp up) of occupancy values.This is system is able to generate n\nramps based upon the occupancy behaviour. Meanwhile ramps are constrained to three ramps only.\nIt provides three options to handle the ramp mechanisms. predictive, reactive and hybrid.\n\nPredictive\nPredictive is when pure predictions are used to generate the ramp timings. The prediction model\npredicts the maximum and minimum timings and accordingly and it generates the ramp timings.\n\nReactive\nReactive ramps only uses real time data and make decisions based upon the current data.\n\nHybrid\nHybrid ramps combings both predictive and reactive components. The first ramp down is based upon\nthe prediction and then the system switch to the reactive mode.\n The floor level ramps use only predictions(as lower magnitude of the occupancy is very unstable and can cause\nunwanted behaviour in the floor ramps).",
      "The first ramp down is based upon\nthe prediction and then the system switch to the reactive mode.\n The floor level ramps use only predictions(as lower magnitude of the occupancy is very unstable and can cause\nunwanted behaviour in the floor ramps). However, for building level ramps we can use both predictive as well as\nhybrid ramps.\nUsability/ Limitations\nThis is a very stable algorithm which can handle buildings and floors from very low occupancy to very high\noccupancy(building level). It will also take into account comfort index, which will be released in next release.\nLimitation:\nAs such there is no limitation to this algorithm. However, in scenarios of low occupancy, it is better not to use reactive\nmode of ramps.\nInstallation and run\nDownload as zip or clone from the repository. Install dependencies from requirements.txt\nInstall the package with setup:\nPD Research\n2$ git clone git@bitbucket.org:ALPD/predictive_ramps.git\n$ python setup.py install\nuse run_predictive_ramps with following parameters.\ncompany:\nName of the company\nbuilding:\nNamespace for the building.\nprediction_date:\nDate for which the ramp will be run.\nramp_type:\nType of ramp mechanism, reactive, predictive or hybrid (default, hybrid).\nAlong with above required parameters. Following are the optional parameters.\n\n\u2014tz:\nTimezone of the building.\n\n\u2014floor_number:\nFloor number of the building, default is None(whole building).\nIt is advised to run this algorithm after 10 AM to get more accurate results.\nModules\nextremum prediction\nThis module contains methods to pull data for previous three months, train the model to predict extremum timings\nand then build predicted ramp timings. First step is to classify days as weekday or weekend and then use them to\nbuilt training data set for the extremum prediction models.\nfloor level\nThis module works same as the extremum prediction. However, it is optimized for the floor level ramps. This module\ntrains the extremum prediction model and during the prediction time, it pushes the ramp down and ramp up timings in\nthe defined windows.\nn ramps\nThis module generates the n ramps for the building level ramps. First ramp down is always from the prediction.\nFollowed by the first ramp down the algorithm will switch to the reactive mode. However, there is an extra safety of\npredictions around the real time ramp timings.",
      "n ramps\nThis module generates the n ramps for the building level ramps. First ramp down is always from the prediction.\nFollowed by the first ramp down the algorithm will switch to the reactive mode. However, there is an extra safety of\npredictions around the real time ramp timings.\nWhile building the ramp timings, the algorithm consistently looks for the plateaus, if after first ramp down, the\nalgorithm finds that occupancy is not behaving as desired (ie. going dow significantly), it will turn on the plateau flag\nand if the plateau continues till the predicted ramp up time, algorithm will fall back on the predictions mode.\nSimilarly after ramp up if the algorithm finds that occupancy is not going up significantly, if will declare plateau and\nswitch back to the predictions mode.\nutils\nUtils contain the utility methods to save the ramp timings report.\nmain\nThis module builds the pipeline between the user input and the ramp timings.\nInput to this module is the run parameters and output is a json with following fields.\nPD Research\n3\ncompany\nName of the company.\n\nns\nNamespace of the building.\n\ndate\nDate of prediction.\n\nfloor\nFloor number.\n\noutput\nOutput is a ISO format time for ramp timings with key value pair as ramp_down_i: time or ramp_up_i: time\nfor i in (1\u2026.n)\nRequirement\n\nInput type\nHuman Occupancy [net occupancy unit of time]\n\nGranularity of data\nMinimum required granularity of data is 15-min. Higher granularity up to 5-min might improve the accuracy\nof the algorithm.\n\nSize\nMinimum size requirement before first implementation is 15 days of data. The variation of error (or\nuncertainty) with density of data is as follow.\nFollowing figures shows variation of first ramp down error with respect to data.\nPD Research\n4Following figures shows variation of first ramp up error with respect to data.\nPD Research\n5Following figures shows variation of second ramp down error with respect to data.\nPD Research\n6Examples\nThe notebook for this package can be found here.",
      "PD Research\n4Following figures shows variation of first ramp up error with respect to data.\nPD Research\n5Following figures shows variation of second ramp down error with respect to data.\nPD Research\n6Examples\nThe notebook for this package can be found here.\nErrors and Solutions\nECM#4: Peak Demand Prediction\nOverview\nAlgorithm\nRequired Data\nRequired BMS Programming\nECM#5: Ramp-Based Peak Management (Game of Fans)\nOverview\nDemand-Side Management (DSM)\nDemand Side Management (DSM) is essentially the management of behavior at the demand-side of energy for\ndifferent goals such as minimizing consumption and reducing peak demand through various predictions, plannings\nand controllings. There are many ways to classify or group DSMs, one of which is shown in the figure below:\nPD Research\n7Expanding on Nantum Ramps\nWith current ECMs for recommended and automated mid-day and end-of-day ramps, the potential to expand the\nramps as demand management measures from startup to shutdown could provide additional reductions on peak\ndemand as well as overall consumption visualized in the figure below. The idea is to coordinate all AHU operations to\nreduce demand while maintaining the allowable zone temperatures with predictions, planning, and constant\nmonitoring through Nantum with data and optimization.\nAlgorithm\nSpace Temperature model\nReturn air temperature for each AHU is chosen as the indicator temperature for all spaces served by this specific\nAHU. Thus, to understand how space temperature will change if the fans are enforced into the ramp, we modeled the\nworst case scenario with the observed data from normal shutdown when the HVAC is turned off. Separate behaviors\nare detected and modeled for winter(heating behavior) and summer(cooling behavior) as shown in the two examples\nbelow.\nPD Research\n8The temperature behavior after the detected shutdown can be understood as the unconditioned behavior of the\nspaces during which no active cooling or heating is provided and the building as a thermal mass is only transferring\nheat with the outside environment.\nOptimization Formulation\nNext, considering the AHU information with fan specifications (Horsepower/efficiency) are available, the\nimportance(weight) of AHUs can be added since a larger fan would have a greater impact on the power and energy\nconsumptions. To quantify the importance of each AHU, the most straightforward way is to assign weight\nproportionally to the size.",
      "To quantify the importance of each AHU, the most straightforward way is to assign weight\nproportionally to the size. Thus, the optimization formulation can be modified as follow:\nThe optimization method used to solve this problem is Particle Swarm Optimization(PSO). As a population based\nalgorithm, PSO provides the opportunity to incorporate black-box models in constraints and objectives for the\nmixed-integer nonlinear optimization problem and generates a sub-optimal solution within a reasonable amount of\ntime.\nPD Research\n9The initial implementation simplifies the process and reliefs the computational burden by running the schedule\noptimization function ahead of time. The optimized schedule is stored and implemented throughout the Fan DSM\nPeriod with the additional checkpoint every timestep to check if the real-time return air temperature would be out of\ncomfort limit through the schedule safety check function, especially the upper bound, if the planned action is taken. If\nspace temperature will exceed or very probable to exceed the comfort limit while the planned action is ramping\ndown, the status for that AHU should be changed back to normal. Instead of iteratively rerun the optimizer, the\nrule-based checkpoints can be much more efficient to implement even though it might miss some potential extra\nsavings compared to if the optimizer is running iteratively throughout the day. The initial schedule generated at the\nbeginning of the day needs to be stored. At the same time, the actual implemented schedule needs to be stored as\nwell for saving validation and other purposes.\nPD Research\n10Demand Response (Reduction) Orders\nFurthermore, similar to critical time peak management, if demand response commands are required, the system can\nalso plan accordingly within a short notice time.\nRequired Data\nRequired BMS Programming\nLink to Full Documentation\nECM#5 Game of Fans\nECM#6: Chilled Water Pump Staging\nOverview\nCommonly seen in Chilled Water(CHW) systems are multiple pumps that are installed in parallel, in series, or a more\ncomplex layout. Normally BMS operation logic pairs pumps with chiller operations for simplicity, but opportunities\nexist especially for parallel pumps to run more at the same time to lower the speed and power consumption.\nECM#6 CHW Pump staging is designed to optimize the number of water pumps that should be turned on to minimize\nthe power consumption to pump the required chilled water flow and differential pressure.",
      "ECM#6 CHW Pump staging is designed to optimize the number of water pumps that should be turned on to minimize\nthe power consumption to pump the required chilled water flow and differential pressure.\nPD Research\n11Algorithm\nRequired Data\nRequired BMS Programming\nLink to Full Documentation\nECM#6 CHW Pump Staging\nECM#7: Condenser Water Pump Staging\nOverview\nSimilar to ECM#6, ECM#7 Condenser Water(CW) Pump staging is designed to optimize the number of CW pumps\nthat should be turned on to minimize the power consumption to pump the required condenser water flow and\ndifferential pressure.\nPD Research\n12Algorithm\nRequired Data\nRequired BMS Programming\nECM#8: Condenser Water Flow Optimization\nOverview\nAlgorithm\nRequired Data\nRequired BMS Programming\nECM#9: Condenser Water Supply Temperature Optimization\nOverview\nAlgorithm\nRequired Data\nRequired BMS Programming\nECM#14: Static Pressure Optimization\nOverview\nRamps have the potential to be expanded over the whole operational hours as long as the temperatures are\nmaintained within a reasonable range defined by building operators or occupants. This can be achieved by activating\nramp (reduced static or direct speed control of AHU fans) multiple times during the day to generate energy reduction\nwhile maintaining the temperature within comfort limits.\nAlgorithm\nThe optimal static pressure setpoint output will be based on a hybrid of one base behavioral model and a\nhigh-resolution trend model.\nUsing AHU base model\nPD Research\n13Figure on the left shows the base model characterizes the expected damper positions under different static\npressures and building load conditions. It is expected that the shape of the model is maintained under different load\nconditions. Thus, with a base model that characterizes the shape of the model from experimental and later, historical\ndata. At real time, the exact adjusted model can be moved from the base model based on the differences between\nthe expected (blue dot) and observed (black dot) damper position at the observed static pressure. Similarly on the\nfan speed model shown on the right side, it is modeled and adjusted as well.\nNext, based on the current damper position and fan speed, the optimal static pressure setpoint is determined based\non the adjusted model and the objective damper position and fan speed determined to provide the optimal static\npressure setpoint.",
      "Next, based on the current damper position and fan speed, the optimal static pressure setpoint is determined based\non the adjusted model and the objective damper position and fan speed determined to provide the optimal static\npressure setpoint. While optimal static is determined directly, the step taken will be conservative towards the optimal.\nUsing AHU high-resolution model\nIf a high-resolution model is allowed to be activated, a high-resolution rolling model will be run if enough valid\nhistorical data is available and if the current system is already operating close to boundary conditions. The\nhigh-resolution rolling model can be visualized as this:\nPD Research\n14With the change of trend of the behavior, next the high-resolution trend model can take over to fine-adjust static to\nmaximize damper opening to achieve lowest energy consumption on AHU fans.\nRequired Data\nMinimum Requirements\n\nStatic pressure reading\n\nStatic pressure setpoint (writable)\n\nCritical damper positions\n\nAHU Fan speed\nAdditional Requirements\nRequired BMS Programming\n\nECM is designed for VAV systems\n\nDampers and fan speed adjust based on the static setpoint\n\nCFM is set automatically by BMS\nLink to Full Documentation\nECM#14 Static Pressure Optimization\nPD Research\n15ECM#16: Chilled Water Flow Optimization\nOverview\nReducing Chilled Water(CHW) flow (increasing the chilled water dT) reduces both chiller and pump power\nconsumption because of the increased chiller COP and decreased pumping energy required. ECM#16 Chilled Water\nFlow Optimization optimizes the chilled water flow rate / differential pressure to reduce chiller and pump power\nconsumption while satisfying the required building cooling load and considering the impact on the AHU fans\u2019 power\nconsumption by monitoring CHW valves, supply air temperatures, and interior space temperature.\nAlgorithm\nRequired Data\nRequired BMS Programming\nLink to Full Documentation\nECM#16 CHW Flow Optimization\nECM#17: NANTUM Automated Demand Management (ADM)\nOverview\nECM#17 Nantum Demand Management is designed to adaptively predict the peak demand and set demand limits\nthroughout the billing cycles to limit the peak demand to reduce peak demand cost on peak demand charge.\nThe expectation is to notify buildings for the rare or abnormal high peaks. Building engineers can take actions to\nsatisfy the limit rather than overshooting.",
      "The expectation is to notify buildings for the rare or abnormal high peaks. Building engineers can take actions to\nsatisfy the limit rather than overshooting. The expected outcome of the ECM is shown below:\nPD Research\n16PD Research\n17Algorithm\nOverview\nPD Research\n18Required Input for the ECM\n\nExpected daily peak for all remaining days for current billing period.\n\nExpected daily peak demand for previous 30 days (generated at the beginning of the 30 days).\n\nActual demand profile during the previous days(historical days need to be checked to eliminate DR days).\nShort and Long Term Demand Prediction\nIn this step, the algorithm of ECM#4 uses historical data, day-type, and weather forecast to provide * Expected daily\npeak for all remaining days for current billing period * Expected current day peak. Here, we also output the previous\n30 days demand prediction generated and stored by the \u2018Peak Demand Prediction\u2019 algorithm based on outdoor\nweather and day-type!\nGenerating Optimal Demand Limit\nThe optimal level based on the expected and actual demand from the previous 30 days will be optimized based on\nthe constraint inputs of Demand Reduction time duration constraint and Demand Reduction number of occurrence\nper month constraint. Next, combined with the expected peak demand for the future days, the optimal demand limit\ncan be generated for the remaining billing period.\nMonitoring Demand Limit\nAfter generating the demand limit at the beginning of the day, during the days, these three values regarding the\ndemand needs to be consistently monitored:\n\nDemand Limit: the latest demand limit generated.\n\nObserved Monthly Peak: the recorded monthly peak demand that has been observed.\nPD Research\n19\nPeak Demand Prediction: the peak demand prediction updated throughout the day.\nAt each timestep, the observed monthly peak should be updated based on the latest reading. Out of the six possible\nscenarios for the values of these three numbers, we only need to make sure that the demand limit is at least equal to\nthe observed monthly peak.\nRequired Data\n\nElectricity demand\nRequired BMS Programming\n\nAs recommendation, no BMS programming is required.\n\nTo be fully automated, BMS needs to be programmed to reduce the demand.",
      "Required Data\n\nElectricity demand\nRequired BMS Programming\n\nAs recommendation, no BMS programming is required.\n\nTo be fully automated, BMS needs to be programmed to reduce the demand.\nLink to Full Documentation\nECM#17 Nantum ADM Presentation\nECM#17 Nantum ADM Full Info\nECM#18: Interior Space Temperature Optimization\nOverview\nECM#18 Interior Space Temperature Optimization is designed to direcly optimize the interior space tempreature\nbased on comfort limit, occupancy, loads of the building in both a reactive and a predictive way. This ECM is\napplicable to a wide range of buildings from larger ones with central plants to smaller ones with heat pump units.\nAlgorithm\nStage 0: Occupancy-Driven Temperature Optimization\nThe initial stage of implementing the Int. Space Temp. Optimization is based on occupancy data from the building.\nThe Int. Temp. Optimization will be activated in between the startup and shutdown for which optimal times are\nprovided by the existing algorithms. Int. Temp. setpoints will be continuously adjusted throughout the day to reduce\ncooling demand and consumption of energy at low occupancy.\nA binary point is used to indicate whether the building should activate this ECM or not. If Indicator, run the algorithm\nto output the optimal interior space temperature setpoint. The output of the setpoint temperature is applied to the\nwhole building for all temperature commands. It can be visualized here\nPD Research\n20Stage 1: Predictive Control of Temperature\nStage 2: Demand-Side Management(DSM) with Temperature Control\nRequired Data\nRequired BMS Programming\nLink to Full Documentation\nECM#18 Interior Space Temperature Optimization\nECM#19: Building Preheat Optmization\nOverview\nUnder the time of use tariff structure for steam, buildings are charged by demand during the designated hours of the\nday (peak times). Nantum Preheat Optimization utilizes existing building hydronic systems as thermal energy storage\ndevices to store heating energy and shifting energy demand out of the critical peak times. The ECM is based on an\nexperienced-based and physics-informed learning algorithm including system modeling and data observation in\ndifferent buildings, environments, and scenarios. Through machine learning, optimization, and transfer knowledge\ntechnologies, Nantum Preheat Optimization ECM provides an optimal preheat procedure recommendation and\npotentially automation.",
      "Through machine learning, optimization, and transfer knowledge\ntechnologies, Nantum Preheat Optimization ECM provides an optimal preheat procedure recommendation and\npotentially automation.\nPotential Savings\nPD Research\n21Algorithm\nLink to Full Documentation\nECM#19 Preheat Optimization\nBenchmarks\nEnergy Benchmark\nIntroduction\nThis algorithm is an improvement over benchmarks v1. The problem with previous benchmarks was it used to\ncompare whole day against whole day in the historical data to select similar days. However, the weather profile does\nnot usually match with the whole day and we end up with wrong results.\nTo tackle this problem, new algorithm divides the whole day into slices and then compare slices with slices of the\nhistorical days and select the days which contributes most number of slices. In the end we fir a normal through these\ndays and generate the benchmark as mean of these days.\nDays are also categorized into three categories, type_0(mondays), type_1(mid\nweekdays), and type_2(weekends and holidays).\nSystem diagram\nBenchmarks\n22Usability/ Limitations\nThis package can be used to see how the building behaved previously on similar days and generating curves for that\nday or get similar days.\nLimitation:\nThe performance of this algorithm depends upon the amount of data we have. At least a year of data is required to\ncover all type of weather scenarios.\nInstallation and run\nDownload as zip or clone from the repository. Install dependencies from requirements.txt\nInstall the package with setup:\n$ git clone git@bitbucket.org:ALPD/sliced_benchmarks.git\n$ python setup.py install\nuse run_predictive_demand with following parameters.\ncompany:\nName of the company\nbuilding:\nNamespace for the building.\nresource:\nName of the resource, eg: electric_demand.\ndate:\ndate for which benchmarks is run\nAlong with above required parameters. Following are the optional parameters.\n\n\u2014n_history_days:\nNumber of history days to use, (default is 360).\n\n\u2014n_days_average:\nNumber of days to average on(use Average of these days demand to train demand for current day), default\nis 4.\nModules\ndata_handler\nThis module contains methods for cleaning the data and slice each day data in six slices. early_morning,\nmorning_transition_1, morning_transition_2, day_time, evening_transition, night_time.",
      "Modules\ndata_handler\nThis module contains methods for cleaning the data and slice each day data in six slices. early_morning,\nmorning_transition_1, morning_transition_2, day_time, evening_transition, night_time.\nBenchmarks\n23logic\nThis module takes sliced data returned by the data handler module and calculate euclidean distance between the\nslices of the current day and the historical days.\nIt takes in the company, building, timezone, start_date(training data start date), end_date(training data end date),\ngran_in (granularity of th data), current_day, selected_day(day selected based upon weather similarity) and returns\nmodels to predict maximum value of the demand. This module also synthesize data using normal distribution.\nmain\nThis module takes all of the above modules and builds the pipeline for fetching, cleaning data, and sorts the slices\nreturned by the logic module according to distance.\nFollowing these operations, the day selection is weighted by number of slices selected from a particular day.\nOnce the days are selected it fits a normal through the days and generate three time series with mean of the\ndistribution, upper bound and lower bound.\nInput to this module is the run parameters and output is a json with following fields.\n\ncompany\nName of the company.\n\nns\nNamespace of the building.\n\ndate\nDate of prediction.\n\nfloor\nFloor number.\n\noutput\nOutput is a dictionary of dictionaries with key value pairs as {time: value} Time is in ISO format in utc\ntimezone. Along with the dictionaries, output has another key named benchmark_days which is a list of\nselected days.\nRequirement\n\nInput type\nDemand [consumption per unit of time]\n\nGranularity of data\nMinimum required granularity of data is 15-min. Higher granularity would not impat the accuracy of the\nalgorithm.\n\nSize\nMinimum size requirement before first implementation is 2 weeks of data. The variation of error (or\nuncertainty) with density of data is shown below.\nBenchmarks\n24Examples\nThe notebook for this package can be found here.\nBenchmarks\n25Errors and Solutions\nWater Benchmark\nIntroduction\nThis package is developed to benchmark daily water usage. It uses day type prediction internally and works in two\nstages.",
      "Benchmarks\n24Examples\nThe notebook for this package can be found here.\nBenchmarks\n25Errors and Solutions\nWater Benchmark\nIntroduction\nThis package is developed to benchmark daily water usage. It uses day type prediction internally and works in two\nstages.\n\nStage 1:\nThis stage is executed when the run time of the algorithm is less than (before) the day type decision time\n(Check day type prediction for this). Algorithm looks at the weather forecast for the input day and look for\nthe most similar weather days in historical data.\n\nStage 2:\nThis stage is executed when the run time of algorithm is greater than the day type decision time. At this\nweather data is not considered to find the similar day. Instead, water consumption between \u201800:00\u2019 and run\ntime is used to find most similar days in historical data.\nDecision behind using water consumption to generate more accurate benchmark was based upon the fact that water\nis used in a different way than other resources. Water is used from a tank. So, consumption curve of water actually\nshows the consumption behaviour by the tank. So, the morning segment of curve shows the level of the water in the\ntank.\nUsability/ Limitations\nThis package can be used to benchmark any resource where today\u2019s consumption is dependent on final portion of\nprevious day.\nLimitations:\nN/A\nInstallation and run\nDownload as zip or clone from the repository. Install dependencies from requirements.txt\nInstall the package with setup:\n$ git clone git@bitbucket.org:ALPD/water_prediction.git\n$ python setup.py install\nThis package takes following arguments.\ncompany:\nName of the company\nbuilding:\nNamespace for the building.\ndate:\nCurrent day date(date for which you want to predict demand.).\nrun_time:\nRun time of the algorithm format(hh:mm:ss).\nAlong with above required parameters. Following are the optional parameters.\n\n--floor_number\nFloor number for which the demand will be\nforetasted.\n\n--use_prediction\nWhether to use prediction or build benchmark by\naveraging selected days. False by default.\nBenchmarks\n26Modules\ndata_handler\nThis module contains following method.\n\nload_dataset\nThis method is responsible for loading resource data.\n\nselect_similar_days\nThis method filter the input days based upon weather profile or previous water consumption and returns top\nn most similar weather days.",
      "Benchmarks\n26Modules\ndata_handler\nThis module contains following method.\n\nload_dataset\nThis method is responsible for loading resource data.\n\nselect_similar_days\nThis method filter the input days based upon weather profile or previous water consumption and returns top\nn most similar weather days. It uses Euclidean distance to calculate day similarity.\n\nprepare_rolling_data\nPrepares training data for time series prediction.\ntrain\nThis module uses the same type days (ref: day type selection), similar days filtering of data_handler module to clean\ndata, build training data and XGBoost model to train model for peak demand prediction. It has following methods.\n\ntrain_model\nReturns trained model for peak prediction\n\npredict_consumption\nTakes trained model and water chunk to predict the water consumption for rest of the day.\nutils\nThis module is responsible for saving predictions . It saves output report as JSON file.\nmain\nThis module uses all the modules eg: data handler, utils, and train modules and build a benchmark or (training\npipeline for pulling data, training, prediction and saving the predictions) based upon user preference.\nRequirement\n\nInput type\nDemand [consumption per unit of time] Human Ocupancy [net occupqncy per unit of time]\n\nGranularity of data\nMinimum required granularity of data is 15-min. Higher granularity would NOT improve the accuracy of the\nalgorithm.\n\nSize\nMinimum size requirement before first implementqation is 15 days of data. The variation of error (or\nuncertainty) with density of data is as follow.\nBenchmarks\n27Examples\nThe notebook for this package can be found here . and here\nErrors and Solutions\nOccupancy\nWi-Fi Occupancy\nOverview\n1.\nNantum analyzes data from building systems and combines this with data from third-party sources, such as IoT\nsensors, to prescribe operational adjustments that improve building performance. One of the most important\ntypes of data Nantum incorporates is occupancy.\n2.\nKnowing how many people are in a building is crucial to safety protocols and calibrating efficient operations\nwithin the building. If the building has a low number of people, the HVAC load may be reduced to save energy.\n3.\nNantum can obtain occupancy counts and trends via buildings with Cisco Meraki WiFi solutions by:\n1.\nIntegrating directly with Cisco Meraki\u2019s cloud based occupancy solution.\n2.",
      "If the building has a low number of people, the HVAC load may be reduced to save energy.\n3.\nNantum can obtain occupancy counts and trends via buildings with Cisco Meraki WiFi solutions by:\n1.\nIntegrating directly with Cisco Meraki\u2019s cloud based occupancy solution.\n2.\nReceiving seen-devices data from the Cisco Meraki Cloud into Nantum.\n3.\nRunning Nantum algorithms to determine the occupancy based off of the seen-device count.\n4.\nSee below diagram for an overview of how the Cisco Meraki Wifi Occupancy feature works.\nOccupancy\n28Link to Full Documentation\nCisco Meraki WiFi Occupancy\nOccupancy\n29Human Comfort\nComfort Index\nIntroduction\nComfort index takes into account current occupancy and interior temperature and provides with a single number\nbetween 0 and 100.\nThe formulation of comfort index is as follow\noccupancy weight = 1 - (1-(e^current_occupancy)/max_occupancy)\nminimum interior temperature in comfort index is\nmin temp = 100 x e^(-0.033 x (Tavg- T*avg)^4)\nmaximum interior temperature in comfort index is\nmax temp = 100 x e^(-0.033 x (Tmin- T*min)^5)\nComfort index = occupancy weight x (a1x min temp + a2x max temp)\nWhere a and b are tuning parameters.\nAt the beginning of the day (in absence of occupancy), comfort index will be 100, as the there is no need of heating\nor cooling if no one is in the building. However, as people start walking in into the building, the comfort index value\nwill change accordingly.\nThis package runs in real time and generates real time value for the comfort index. Any value greater than 90 can be\nconsidered as good value of comfort index.\nInstallation and run\nInstall the package with setup:\n$ git clone git@bitbucket.org:ALPD/comfort_index.git\n$ python setup.py install\nThis package takes following arguments.\ncompany:\nName of the company\nbuilding:\nNamespace for the building.\ndate:\nDate for which the comfort index will be run\nIn addition to these arguments it takes following optional arguments.\n\u2014tz\nTimezone of the building , bby which pull from db.\n\u2014time\ntime at which algorithm is run.\n--floor\nFloor number in the building.\nModules\nData handler\nThis module is responsible for loading and cleaning data.",
      "\u2014tz\nTimezone of the building , bby which pull from db.\n\u2014time\ntime at which algorithm is run.\n--floor\nFloor number in the building.\nModules\nData handler\nThis module is responsible for loading and cleaning data.\nlogic\nThis module is responsible for following tasks\nHuman Comfort\n30\ncalculate average and max interior space temperature\n\ngenerate occupancy weight using following equation:\noccupancy weight = 1 - (1/(e^current_occupancy)/max_occupancy) current occupancy is real time occupancy.\nmax occupancy can be calculated as the average maximum occupancy of previous days for eg: in previous\nmonth.\nmain\nThis module uses the data handler and logic module to generate comfort index value.\nIt takes occupancy weight from the logic module. It uses multiplication of occupancy weight with difference of upper\nbound, target temperature.\nOnce the value is generated it uses utils module to save the value in an output report.\nRequirement\n\nInput type\nHuman occupancy [net occupancy per unit of time] Interior Temperature [in F] (or Return Temperature [in\nF])\n\nGranularity of data\nMinimum required granularity of data is 15-min. Higher granularity would not improve the accuracy of the\nalgorithm.\n\nSize\nMinimum size requirement before first implementqation is 1 day of data. The algorithm is not sensitive to\nsize of data.\nExamples\nDownload python notebook from here..\nErrors and Solutions\nSpace Utilization\nUsing a Mobile Robot\nOverview\nSpace utilization with mobile robots provides a mean to digitalize work spaces in an efficient way. The process\nenables nonintrusive occupancy monitoring and space utilization analysis leading to huge oppotunites in energy\nsavings and human resource management.\nRobot Selection\nThe robot platform chosen is from XiaoR Geek. XiaoR Geek The robot has a NVIDIA Jetson Nano computer onboard\nwhich provides great computational power for ML and AI algorithms. Sensors on the robot includes IMU, LiDar and\ncamera with the potenial to expand for an robotic arm and other types for sensors for different purposes. The\ntank-style continuous track makes the robot stable through small bumps and different types of floors.\nProcedures\n1.\nStep 1 is to generate the floor map for the space with a defined coordiante. The boundaries and interior\ndivisions are also defined in the map.\n2.",
      "The\ntank-style continuous track makes the robot stable through small bumps and different types of floors.\nProcedures\n1.\nStep 1 is to generate the floor map for the space with a defined coordiante. The boundaries and interior\ndivisions are also defined in the map.\n2.\nStep 2 is to generate a plan of sampling points to be reached within the map for RSSI data gathering.\nSpace Utilization\n313.\nStep 3 is to construct the RSSI models for each access point from which the location within the map can be\nrelated to the signal strength of RSSI.\n4.\nStep 4 is to reverse the RSSI models from all available access point so that for any device with given RSSIs\nfrom all access points, the location of the device can be calculated.\nInitial Results\nFloor Mapping with SLAM\nPerforming SLAM over the space translates the whole space into a 2D floorplan with a defined coordinate system.\nThe map can be cleaned and calibrated to better define the wall locations and boundaries.\nSpace Utilization\n32Locational Sampling\nAfter generating the map, points are sampled uniformly across the space and those coordinates are set for the robot\nto move to for data gathering.\nRSSI Modeling\nAfter gathering enough data at each sampling points, RSSI models are constructed for each Access Point as shown\nin the two examples below for AP2 and AP4-Sales.\nSpace Utilization\n33Space Utilization\n34Space Utilization with RSSI model\nTesting using one day of data, it can be visulized for on indicator device shown in the first figure below and for all the\ndetected occupants\u2019 space utilization intensity shown in the second figure below.\nSpace Utilization\n35Space Utilization\n36Monitoring and Measurement\nNANTUM Potential Saving Calculator\nNantum Potential Savings is an application designed to calculate the estimated monthly potential savings with\nminimum information needed.\nMonitoring and Measurement\n37With at least one month of the electricity data and the expected schedule, the calculator has the ability to estimate\nECM#3: startup and shutdown savings. If additional fan information is available, the calculator can estimate ECM#2:\nmid-day and end-of-day ramp savings. The calculator also has a simple User Interface which allows simple usage of\nthe package shown in the figure below.\nMonitoring and Measurement\n38The calculator will generate a spreadsheet containing the daily, monthly, and yearly savings if available.",
      "The calculator also has a simple User Interface which allows simple usage of\nthe package shown in the figure below.\nMonitoring and Measurement\n38The calculator will generate a spreadsheet containing the daily, monthly, and yearly savings if available. At the same\ntime, an example of the summarized plot showing the monthly savings in kWh and percentage for each chosen\nECMs is shown here.\nMonitoring and Measurement\n39A video demo using the calculator can be found here:\nDemo NANTUM Potential Saving Calculator\nEnergy\nChiller Retrofit - Steam Turbine to Electric\nOverview\nAs the trend of elctrification continues in buildings, most building have planned or partly retrofited chiller plant to\nusing electric chillers if not fully retrofited yet. This document answers the following quesiotns:\n1.\nHow does the efficiency of steam driven chiller compare to electric chillers?\n2.\nAs an energy source, how are the costs of electricity compare to steam?\n3.\nAs for operation, how are the costs of electric chillers compare to steam driven ones?\n4.\nLastly, how should the building operate with a mix of steam and electric chillers to be most cost-effective?\nEfficiency of Steam and Electric Chiller\nEfficiency, or reversely, Coefficient of Performance(COP) is commonly used to quantify the performance of chiller.\nCOP can be understand the coefficient relats how much energy input to generate the desired useful output as shown\nbelow:\nCOP = Refrigeration Provided/Energy Demanded = Output/Input\nIn general, electric chillers now can reach over 5 to 6 COP at the designed load conditions. Meanwhile, old steam\ndriven turbine chillers are consistently unable to reach COP of 1, indicating essentailly more than one unit energy\ninput is needed to provide one unit of useful output work. Two typical COP curve can be visualized here:\nEnergy\n40Cost of Steam and Electric\nIn general, for the same amount of energy, steam is a about half of cost of electric as shown in the table below:\nEnergy\n41Combining with the COP shown above, an example of a 900ton chiller running using steam versus using chiller at\ndifferent load conditions (assuming 8 hour run time) is shown here:\nEnergy\n42In general, to provide the same amount of cooling, steam cost over 2 times more than electric.",
      "A rare scenario which makes using steam reasonable is during a period when building is incentivized to reduce\neletric demand. The 0 demand charge for steam during summer makes it a suitable backup energy source during\nthose critical times.\nExample: Switching to Electric (345 Park Avenue, 2019)\nUsing the 2019 data of 345 Park Avenue which runs fully on steam driven chillers and electric chiller models, an\nanalysis of energy and cost changes to the building is conducted to show the potential of switching from steam to\nelectric fully. The change to steam and electric demand profile can be visualized here:\nEnergy\n43By switching from steam to eletric, the CO2 emission reduces over 8000 ton, which is over 18% of the building\nemission. This can potentially aviod about $800k penalties limited by LL97.\nEnergy bill wise, the total cost for steam and electricity is reduced by over $550k, which is about 8% of the building\u2019s\ntotal cost of energy.\nHow Should Buildings Operate with A Mixed of Both chillers\nECM#10 Chiller Staging is designed to tackle such problem. NANTUM will combine the capability of ECM#4 demand\nprediction and efficiently constructed chillers models to send optimal recommendation to chiller choice at the\nbegining of the day before startup as an example shown in the figure below. For more detail, please visit ECM#10\nChiller Staging page.\nEnergy\n44Greenhouse Gas (GHG)\nLocal Law 97\nLocal Law 97 (LL97) is a part of the Climate Mobilization Act which aims to drastically reduce carbon emissions from\nbuildings. The goal is to ultimately reduce carbon emission by 80% by 2050 comparing to 2005 baseline. A summary\nof the LL97 from Urban Green Council can be found here: (LL97 Summary).\nNANTUM GHG Emissions App\nOverview\nThe Green House Gas (GHG) Emissions app allows users to track their building\u2019s GHG emissions against GHG\nlimits set by regional legislation (e.g. NYC Local Law 97). It\u2019s a sort of navigation tool for building engineers and\nmanagement to follow to keep their space within their GHG emission limits.",
      "NYC Local Law 97). It\u2019s a sort of navigation tool for building engineers and\nmanagement to follow to keep their space within their GHG emission limits. GHG benchmarks within the app are\ngenerated based on weather information and GHG emissions limit (calculated based on building size and GHG\nmultiplier)\nNANTUM UI\nDaily Metric Ton/Hr Widget provides hour by hour tracking (ploted in green) for the engineer to follow against either\nthe 2024 benchmark (ploted in red) or the 2030 benchmark (ploted in blue).\nGreenhouse Gas (GHG)\n45Daily Metric Ton provides hour by hour cumulative total of GHG emissions for the day.\nGreenhouse Gas (GHG)\n46Current Year Monthly View provides monthly tally of GHG emissions for each month of the calendar year. For\nmonths that have not completed, a GHG Predicted Readings bar will display indicating the expected amount of GHG\nemissions for that month.\nGreenhouse Gas (GHG)\n47Average GHG Multiplier\nEPA (EPA eGrid Data) provides the average fuel mix for different zones in New York State.\nBased on the average fuel mix, the GHG multiplier NANTUM uses to calculate the GHG emission is shown in the\nequation below for electrcity and steam respectively:\nkgCO2_e = kWh * 3.412 * 0.08469\nkgCO2_e = Mlbs * 1194 * 0.04493\nReal-Time GHG Multiplier\nNYISO Real-Time Data\nWhile NYISO\u2019s Real-Time Dashboard NYISO Real-Time Dashboard provides data for real-time fuel mix across New\nYork State (shown as an example in the figure below), the fuel mix and the carbon intensity to each zone has not\nbeen made public.\nGreenhouse Gas (GHG)\n48Greenhouse Gas (GHG)\n49WattTime\nWattTime, using data from power print and grid operator to estimate the Marginal Operating Emissions Rates\n(MOERs), has the capability to provide valid estimation on the carbon emission intensity for zones within the state.\nWith the subscription, historical and forecast data can also be available for use. The introductory material for\nWattTime can be found here: WattTime Introduction.",
      "With the subscription, historical and forecast data can also be available for use. The introductory material for\nWattTime can be found here: WattTime Introduction. All people can access a scorce of grid emission intensity\nrepresenting the cleaness of current energy generation compared to the past month (shown in the figure below),\nwhile subsribed users can access the MOER values.\nGreenhouse Gas (GHG)\n50Carbonara, Singularity Energy\nSimialar to WattTime, Carbona provides historical, current and future grid carbon intensity with the resolution to zone\nlevel. Link to Carbonara website can be found here: Carbonara.\nMeasuring Building GHG Using Real-Time Multiplier\nComparing the 2024 Local Law 97 carbon intensity multiplier and the real-time carbon intensity multiplier, two typical\ndays are shown below:\nGreenhouse Gas (GHG)\n51The scenario of higher average multiplier and higher real-time multiplier can be both observed. Using an example\nbuilding kW profile, the comparison of GHG emission can be plotted as shown:\nGreenhouse Gas (GHG)\n52Comparing with the whole year of 2019 for the example building of about 1,900,000 square footage, the kg of CO2\nemission calculated for the building is significantly lower by using the real-time GHG multiplier factoring the time of\nthe use of the electricity shown in the figure below:\nGreenhouse Gas (GHG)\n53The cost the building could potentially avoid by switching to real-time GHG multiplier is over 430 thousand dollar.\nAnomaly Detection\nAnomaly Detection\nEarly anomaly detection and diagnosis in commercial buildings can significantly decrease energy wastage and\noccupancy discomfort, and improve system reliability and equipment life. To increase building resilience and prevent\ncostly incidents, Concurrent Anomaly Detection and Diagnosis (C-ADD) predicts anomaly behavior by detecting the\nconditions that usually lead to them and also predicts the potential future impacts on other equipment and features.\nC-ADD in V1 is run in near real-time, within 5 min of receiving data.\nFeature Anomaly Detection\nFeature Anomaly, control the quality of measured data, and improves the robustness of data analysis under\npresence of noise and faulty sensors.\nIt finds the measured data that do not follow the normal pattern dynamically for building operation behavior. It also\nfinds the malicious sensors that always generate outlier values, and not-active sensors. It can also ensure the\nsecurity of the measured data.",
      "It finds the measured data that do not follow the normal pattern dynamically for building operation behavior. It also\nfinds the malicious sensors that always generate outlier values, and not-active sensors. It can also ensure the\nsecurity of the measured data.\nAnomaly Detection\n54Time-Series Predictive Anomaly Detection\nIn a time series anomaly, the value and the rate of change of the time series data measured are controlled. The\nalgorithm is designed to hierarchically monitor and predict the expected behavior of the target time series (i.e., value\nand rate of change) at different times of the day, different types of day ,different level of occupancy, and different\noutdoor conditions (i.e., weather).\nThe Algorithm Evaluates:\n1.\nCurrent values and value rate of change\n2.\nDay of the week and holidays\n3.\nTime of day\n4.\nExterior weather conditions\nSupported Resources:\n\nInterior + Perimeter space temperature,\n\nElectric demand + consumption\n\nVoltage, current, power\n\nCo2 level\nAnomaly Detection\n55\nWater Consumption\n\nOccupancy\nMultivariate Anomaly Detection\nNantum multivariate anomaly detection algorithm is a smart high-dimensional rule-based time series anomaly\ndetection algorithm. It is designed to improve time series anomaly detection by considering the impact of other\nmeasured and setpoint time series data in detecting anomalies in a target time series for different scenarios As an\nexample, considering the impact of a secondary water temperature setpoint in predicting the expected behavior of\nthe perimeter temperature during the day.\nNANTUM Common Modules\nDay Type Prediction\nIntroduction\nThis package is developed to overcome the limitations of exceptional days. As, someone need to actively update\nexception days and often result is not as expected. To overcome these limitations, we developed type of the day\nprediction. The first version of this algorithm looks at the occupancy between 5 AM and 11 AM and determines the\nbest time at which it can decide type of day it is.\nThis algorithm takes into account occupancy at given timestamp and looks at the correlation between the occupancy\nat that timestamp and maximum occupancy. If the correlation is more than the threshold. It will cluster historical\noccupancy values at that timestamp into n clusters and then determines to which cluster the target day belongs.\nIt provides both time at which decision is made and the type of the day.",
      "If the correlation is more than the threshold. It will cluster historical\noccupancy values at that timestamp into n clusters and then determines to which cluster the target day belongs.\nIt provides both time at which decision is made and the type of the day.\nUsability/ Limitations\nThis package can be used in any case where we are classifying days in holidays, weekdays and weekends. The\nalgorithm can understand these patterns from occupancy and resource utilization.\nLimitations:\nThis algorithm can not be used as it is for all the cases. As, different resources can have their own day (class types).\nCare must be taken while using this algorithm.\nInstallation and run\nDownload as zip or clone from the repository. Install dependencies from requirements.txt\nInstall the package with setup:\n$ git clone git@bitbucket.org:ALPD/daytype_prediction.git\n$ python setup.py install\nuse daytype_prediction.main\u2019 s get_day_predictions method get historical day types, decision time and cluster\ncenters.\nThis method takes following arguments.\ncompany:\nName of the company\nbuilding:\nNamespace for the building.\nfloor:\nParticular floor for which decision is to be made.\nresource:\nName of the resource, eg: Occupancy.\nNANTUM Common Modules\n56start_date:\nStart date for the dataset\nend_date:\nEnd date of the dataset.\ntz:\ntime zone of the building.\nAlong with above required parameters. Following are the optional parameters.\n\ncorr_required:\nrequired correlation between current and the maximum resource value. Default is 0.9.\nModules\ndata_handler\nThis module contains following method.\n\nload_dataset\nThis method is responsible for loading resource data.\n\nocc_at_time\nReturns the occupancy at given timestamp.\n\nfind_the_time_with_accepted_corr_time\nReturn the time at which correlation between the resource value and the maximum value of resource is\ngreater than accepted value.\n\nday class\nReturns the day type for the input day.\nsame_class_days\nThis module takes any input day and returns the time and type of days from history which are of same day type as\nthe input day.\n\nReturn day attributes, eg: type of the day, decision time, cluster centers.\n\nReturns days with same day class.\nRequirement\n\nInput type\nHuman Occupancy [net occupqncy per unit of time]\n\nGranularity of data\nMinimum required granularity of data is 15-min. Higher granularity would NOT improve the accuracy of the\nalgorithm.",
      "\nReturns days with same day class.\nRequirement\n\nInput type\nHuman Occupancy [net occupqncy per unit of time]\n\nGranularity of data\nMinimum required granularity of data is 15-min. Higher granularity would NOT improve the accuracy of the\nalgorithm.\n\nSize\nMinimum size requirement before first implementation is 1 month of data. The variation of error (or\nuncertainty) with density of data is provided here [The link will be available by Jan. 30, 2019]!\nExamples\nThe notebook for this package can be found here..\nNANTUM Common Modules\n57Errors and Solutions\nBilling Engine\nIntroduction\nRates engine can be used to utility bills for any resource. It has two components, rates schema and rates engine.\nRates schema stores the utility billing rules for the utility provider. The rates engine is responsible for processing\nthese rules, and apply the rules from the rates db to the measurement data(demand or consumption data) and\ncalculate the bill for the utilities.\nA detailed documentation about rates engine can be found here..\nSystem Diagram\nInstallation\nInstall the package with setup:\n$ git clone git@bitbucket.org:ALPD/rates_engine.git\n$ python setup.py install\nModules\nData handler\nThis module is responsible for loading data for the provided list of meters. Currently, we have methods to load\nconsumption and demand data for provided submeters. This module loads data for the given building and then\nconvert it to building\u2019s timezone. (Since our backend team stores all the data in UTC format.) There is also an option\nto get cumulative data for the entire building or load the data for every sub meter.\nWe are dealing with following types of data currently - Consumption data - Demand data\nRules handler\nThis module is responsible for processing billing rules. Currently rates engine supports following categories of the\nrules. - Consumption rules - Demand rules - Measurement dependent children - Subtotal dependent children -\nConstant rules - Monthly rules - Total dependent children\nRates Logic\nRates logic module is the heart of rates engine, where it combines data with rules and generate the charges. It\nperforms following functions.\n\nChunking the time series with rule.",
      "It\nperforms following functions.\n\nChunking the time series with rule.\n\nValidate chunks\n\nProcess ceil exceeding chunk\n\nDemand calculation logic\n\nConsumption calculation logic\n\nCharge calculation logic\nStatement Builder\nStatement builder creates a statement which is used by our internal Nantum system to display the bills and match\nthem against the electricity statements of the service providers.\nNANTUM Common Modules\n58A typical statement has following fields:\n\naccount_id\n\nns\n\nPeriod_id\n\nRate_structure\n\nService_type\n\nStage\n\nCharges_list\n\nMeasurements\n\nType\n\nCompany\n\nAp_ref\n\nDue_by_date\n\nAmount_due\n\nNew_charges\n\nPrevious_balance\n\nStatement_start_date\n\nTotal_bill_amount\nExamples\nDownload python notebook from here..\nErrors and Solutions\nSimilar Days\nIntroduction\nSimilar days is a package to find the days which are similar to current day in the history based upon the weather\nprofile of the days. It pulls the data in the history and generates the list of ranked days in order of their similarity to the\ncurrent day.\nUsability/ Limitations\nThis package can be used to find days which are similar to current day based upon the outside weather.\nLimitation:\nThis is not an ideal package to be used for any kind of benchmarking.\nInstallation and run\nDownload as zip or clone from the repository. Install dependencies from requirements.txt\nInstall the package with setup:\n$ git clone git@bitbucket.org:ALPD/similar_days.git\n$ python setup.py install\nuse run_similar_days with following parameters.\ncompany:\nNANTUM Common Modules\n59Name of the company\nbuilding:\nNamespace for the building.\nresource:\nName of the resource, eg: electric_demand.\ntarget_date:\ntarget date (date of the day). eg: 2018-09-01.\nAlong with above required parameters. Following are the optional parameters.\n\n\u2014floor_number:\nfloor for which run the algorithm.\n\n\u2014attributes_only:\nWhether to generate attributes only or also common days leave blank if (false) else its true.\n\n\u2014tz:\nTimezone of the building(default: pull from db).\n\n\u2014use_beta:\nWhether to use dev env default is False.\nModules\ndata_handler\nThis module contains methods for pulling weather data and resource data.",
      "\n\u2014tz:\nTimezone of the building(default: pull from db).\n\n\u2014use_beta:\nWhether to use dev env default is False.\nModules\ndata_handler\nThis module contains methods for pulling weather data and resource data. It cleans the data and throw away days\nfor which data is missing for more than three consecutive intervals, followed by calculating the distance among the\ndays.\nThis module can compare days in three different ways:\n\npeak time\nCompares the day for peak time only and returns ranked days according to\nsimilarity.\n\noffpeak time\nCompare day for offpeak time only and returns ranked days according to\nsimilarity.\n\nfull day\nCompare full day and returns ranked days according to similarity.\nAlong with the similar days this module also generates the day attributes (on peak demand, off peak demand, total\ndemand, on peak consumption, off peak consumption, total consumption, NaNs seen in data) for the target day.\nmain\nThis module is responsible for pulling on peak and off peak times for the building. It uses these peak times and pass\nthem to the data handler module and generates the ranked list of the days from the past for full day, on peak, off\npeak times and day attributes respectively.\nThe output report is generated in following format.\n\ncompany\nName of the company.\n\nns\nNamespace of the building.\nNANTUM Common Modules\n60\ndate\nDate of prediction.\n\nfloor\nFloor number.\n\noutput\nOutput is a dictionary with list of ranked days for peak, off peak, full day and list of day attributes for peak,\noff peak and total day.\nutils\nThis module contains methods to save the report in the output directory.\nRequirement\n\nInput type\nDemand [consumption per unit of time] Weather historical data\n\nGranularity of data\nMinimum required granularity of data is 15-min. Higher granularity would NOT improve the accuracy of the\nalgorithm.\n\nSize\nMinimum size requirement before first implementqation is 1 month of data. It is recomended to have full\none-year of data before implementation.\nExamples\nA detailed documentation about rates engine can be found here..\nErrors and Solutions\nTraining Pipeline\nIntroduction\nThis project includes components which stick together to build a training pipeline for Nantum.",
      "It is recomended to have full\none-year of data before implementation.\nExamples\nA detailed documentation about rates engine can be found here..\nErrors and Solutions\nTraining Pipeline\nIntroduction\nThis project includes components which stick together to build a training pipeline for Nantum. These components are\nresponsible for fetching data, cleaning it, taking care of different hyper parameters to be used for training,\nautomatically cut training when it is not improving performance anymore. Save model weights automatically after few\nepochs. Choose the appropriate model etc.\nInstallation and run\nDownload as zip or clone from the repository. Install dependencies from requirements.txt\nInstall the package with setup:\n$ git clone git@bitbucket.org:ALPD/training_pipeline.git\n$ python setup.py install\nuse run_energy_trainer with following parameters.\ncompany:\nName of the company\nbuilding:\nNamespace for the building.\ntrain_end_date:\nEnd date for training data.\nNANTUM Common Modules\n61\u2013resource_list:\nList of the resources eg: electric steam\n\u2013data_granularity:\nConsumption data granularity, default is 5min.\n\u2013measurement_types:\neg: consumption, demand\n\u2013data_transformation:\nType of transformation applied to data. eg: cbrt, sqrt\nModules\ndata_processor\nThis module contains methods for Cleaning data, Segregate data to train, test Normalize data and Recompute\noriginal data.\ncallbacks\nCallbacks are used to perform some task based upon predefined conditions while learning is in progress.\nFollowing are list of callbacks:\nStore weights:\nStore weights everytime model performs better on the validation set.\nLearning rate decay:\nDecrease learning rate if model does not improve for x epochs.\nTerminate training:\nTerminate training if model is not improving for x epochs.\nTensorboard:\nStore learning metadata for visualization and check how training is going.\nmodel\nThis module contains the definitions of the model. eg: number of layers and neurons per layer.\nevaluate\nThis module takes the checkpoints, evaluate them on the test data and save the best weights.\nmain\nThis module provides a pipeline for both training and prediction.\nTraining pipeline builder\nThis method uses the data processor to get training data and model definition from model module, callbacks\nfrom the callbacks module and then use them to train, evaluate and save the best models.\nPrediction builder\nThis method loads the models, fetch and cleans data, predict and save the predicted values.",
      "Training pipeline builder\nThis method uses the data processor to get training data and model definition from model module, callbacks\nfrom the callbacks module and then use them to train, evaluate and save the best models.\nPrediction builder\nThis method loads the models, fetch and cleans data, predict and save the predicted values.\nNANTUM Common Modules\n62Examples\nErrors and Solutions\nTTRCL\nIntroduction\nTTRCL is an aircraft based startup model which divides building operation in following stages:\n\nTaxi\nThis is the phase before startup time when intrior space temperatures are in unregulated stage.\n\nTake off\nTime between the time when operator start the building and till the it reaches the peak value of temperature\nreduction.\n\nRun\nTime between takeoff and time by which temperatures converge.\n\nCruise\nTime when the temperatures are converged and final ramp down.\n\nLanding\nTime between final ramp down process start and building shut down.\nTo perform above operation it goes through following steps:\n\nClassify historical days as cooling vs heating days\nLooks at the historical data and classify the days as either heating day or cooling day looking at slope of\nthe temperatures between takeoff and cruise state.\n\nFind similar interior space temperature days in history\nCompare interior temperature of unregulated part to the historical data and find most similar interior space\ntemperatures.\n\nTrain models for interior space temperature\nTrain XGBoost to predict interior space temperature in both unregulated and regulated stage.\n\nPredict interior space temperature\nPredict interior space temperature for both regulated and unregulated part of the day.\n\nTrain models for electric consumption\nTrain models to predict electric consumption for the startup.\n\nPredict electric consumption\nPredict electric consumption based upon chosen startup time.\n\nAuto SLA temperature detection\nIt uses selected day to find out the SLA temperature.\nThis algorithm also uses first generation of anomaly detection (behaviour anomaly) to filter out sensor readings\nbehaving abnormally. It is also capable of sending advance recommendations ie at \u201808:20\u2019 PM of previous day.\nHowever, the accuracy of the later recommendation will be higher as compared to ones sent in the night time.\nUsability/ Limitations\nThis package is more than just a startup algorithm which models the thermodynamics of the building throughout the\nday. It has built in M&V and this concept can be used throughout the day to make decisions.\nLimitation:\nA week of data is required for the season.",
      "Usability/ Limitations\nThis package is more than just a startup algorithm which models the thermodynamics of the building throughout the\nday. It has built in M&V and this concept can be used throughout the day to make decisions.\nLimitation:\nA week of data is required for the season.\nNANTUM Common Modules\n63Installation and run\nDownload as zip or clone from the repository. Install dependencies from requirements.txt\nInstall the package with setup:\n$ git clone git@bitbucket.org:ALPD/ttrcl.git\n$ python setup.py install\nuse run_ttrcl with following parameters.\ncompany:\nName of the company\nbuilding:\nNamespace for the building.\nprediction_date:\nDate for which the startup will be predicted.\nsla_time:\nSLA time of the building.\nAlong with above required parameters. Following are the optional parameters.\n\n\u2014sla_temp\nSLA temperature of the building.\n\n\u2014sample_day\nSample day to be used for the modeling.\n\n\u2014pos\nPercent of interiors temperature to be in comfort band.\n\n\u2014tz:\nTimezone of the building.\n\n\u2014kwh_start_time\nStart time for KWH calculation.\n\n\u2014temperature source\nSource of the temperature, whether interior space temperature or return air temperature.\n\n\u2014floor_number:\nFloor number of the building, default is None(whole building).\nModules\ndata_handler\nThis module is responsible for pulling data, filtering out anomolous data, normalize, denormalize data, prepare\ninterior space, return air temperature for training and prediction.\ndays_classification\nThis module is responsible for classifying days as heating vs cooling days by looking at slope of the temperatures\nbetween start time and convergence time.\nSimilar_days\nThis module is responsible for finding out similar interior temperature days using interior space temperatuer and then\nfilter out them based upon weather to also take into account effect of weather on interior space temperature.\nNANTUM Common Modules\n64logic\nThis module has methods to find out the historical startup time and convergence timne using fan status or fan speed.\ntrain\nThis module takes regulated chunk taxi, take off and cruise (3 chunks) and returns three models to trained on these\nchunks respectively.\npredict\nThis module takes prediction day start chunk, unregulated model, regulated models, lower and upper bound, heating\nor cooling season as an input an produces startup time.\nThe prediction methods in this package are also reused for electric/ steam consumption prediction.",
      "predict\nThis module takes prediction day start chunk, unregulated model, regulated models, lower and upper bound, heating\nor cooling season as an input an produces startup time.\nThe prediction methods in this package are also reused for electric/ steam consumption prediction.\nsla_temperature\nThis module takes SLA time, selected day as input and returns sla temperature to be used for the prediction.\nunregulated_temperature\nThis module uses interior space temperature data to build a model for interior space temperature in the unregulated\ntime and then predict the interior space temperature for rest of the day using prediction method.\nutils\nThis module contains utility methods to save generate start up timings.\nmain\nThis module takes into account all the above modules and build a pipeline to pull data, clean data, train , predict and\nreport the results.\nRequirement\n\nInput type\nDemand [consumption per unit of time] Interior Temperature [in F] (or Return Temperature [in F])\n\nGranularity of data\nMinimum required granularity of data is 5-min. Higher granularity to 1-min would improve the accuracy of\nthe algorithm.\n\nSize\nMinimum size requirement before first implementqation is 1 month of data. The variation of error (or\nuncertainty) with density of data is provided here [The link will be available by Jan. 30, 2019]!\nExamples\nThe notebooks for this package can be found here.\nErrors and Solutions\nDemand Forecast\nIntroduction\nThis package is developed to forecast demand five days ahead. This includes real time updates in current day\ndemand prediction according to seen behaviour of the demand curve. The algorithm uses day type prediction\ninternally and works in two stages.\nNANTUM Common Modules\n65\nStage 1:\nThis stage is executed when the run time of the algorithm is less than (before) the day type decision time\n(Check day type prediction for this). Algorithm looks at the weather forecast for the input day and predicts\nthe demand for the target day.\n\nStage 2:\nThis stage is executed when the run time of algorithm is greater than the day type decision time. During\npredictions weather data is not considered to train or predict the model. Instead, electric readings at run\ntime for historical data is used to train the model and electric reading at run time for current day is used to\npredict the peak demand.\nDemand for the future days is predicted using weather forecast only.",
      "During\npredictions weather data is not considered to train or predict the model. Instead, electric readings at run\ntime for historical data is used to train the model and electric reading at run time for current day is used to\npredict the peak demand.\nDemand for the future days is predicted using weather forecast only.\nUsability/ Limitations\nThis algorithm is high fidelity model which reacts to decisions made by the operator in real time.\nLimitations:\nThe predictions for the future days are sensitive to the error in the weather forecast error. If direction of the error of\nthe model and weather forecast in the same direction they can add up to larger magnitude. If the errors are in\nopposite directions, they will decrease overall error.\nInstallation and run\nDownload as zip or clone from the repository. Install dependencies from requirements.txt\nInstall the package with setup:\n$ git clone git@bitbucket.org:ALPD/demand_forecast.git\n$ python setup.py install\nuse daytype_prediction.main\u2019 s get_day_predictions method get historical day types, decision time and cluster\ncenters.\nThis method takes following arguments.\ncompany:\nName of the company\nbuilding:\nNamespace for the building.\ndate:\nCurrent day date(date for which you want to predict demand.).\nrun_time:\nRun time of the algorithm format(hh:mm:ss).\ntz:\ntime zone of the building.\nAlong with above required parameters. Following are the optional parameters.\n\n--floor_number\nFloor number for which the demand will be\nforetasted.\nModules\ndata_handler\nThis module contains following method.\n\nload_dataset\nThis method is responsible for loading resource data.\nNANTUM Common Modules\n66\nselect_similar_days_weather\nThis method filter the input days based upon the weather profile of the days and returns top n most similar\nweather days. It uses Euclidean distance to calculate day similarity.\ntrain\nThis module uses the same type days (ref: day type selection), weather based filtering of data_handler module to\nclean data, build training data and XGBoost model to train model for peak demand prediction. It has following\nmethods.\n\nbuild_training_pipeline\nReturns trained model for peak prediction\n\npredict_before_ideal_time\nTrains model based upon weather data and predict demand forecast for input day.\nutils\nThis module is responsible for caching, searching and loading of the preprocessed data and day type attributes. All\nthe output cache files are saved as pickle format.",
      "utils\nThis module is responsible for caching, searching and loading of the preprocessed data and day type attributes. All\nthe output cache files are saved as pickle format.\nIt saves output report as JSON file.\nmain\nThis module uses all the data handler, utils, and train modules and build a pipeline for pulling data, training,\nprediction and saving the predictions.\nNOTE:\nThis module pulls the data at run time and looks at the last time stamp of the pulled data between \u201c00:00 and run\ntime\u201d and uses the last seen timestamp as the run time of the algorithm (In case, there is latency on the gateway side\nand we don\u2019t have data point at run time.).\nRequirement\n\nInput type\nDemand [consumption per unit of time] Weather data\n\nGranularity of data\nMinimum required granularity of data is 5-min. Higher granularity to 1-min would improve the accuracy of\nthe algorithm.\n\nSize\nMinimum size requirement before first implementation is 15 days of data. The variation of error (or\nuncertainty) with density of data is as follow.\nNANTUM Common Modules\n67Examples\nThe notebook for this package can be found here..\nNANTUM Common Modules\n68Errors and Solutions\nPeak Demand\nIntroduction\nPeak demand package can be used to calculate peak demand for given resource from 1st day of billing period to\npassed day (if billing period found for passed day), 1st day of the current month to current date (otherwise). This\npackage is run everyday end of the day and it returns the magnitude of the peak demand as well as datetime at\nwhich that peak was observed.\nUsability/ Limitations\nThis package can be used to find the peak for any resource over given billing period.\nLimitation:\nN/A\nInstallation and run\nDownload as zip or clone from the repository. Install dependencies from requirements.txt\nInstall the package with setup:\n$ git clone git@bitbucket.org:ALPD/peak_demand.git\n$ python setup.py install\nuse run_peak_demand with following parameters.\ncompany:\nName of the company\nbuilding:\nNamespace for the building.\nresource:\nName of the resource, eg: electric_demand.\nend_date:\nend date of the calculation, inclusive.\nAlong with above required parameters. Following are the optional parameters.\n\n\u2014floor_number:\nfloor for which run the algorithm.\n\n\u2014tz:\ntimezone of the building(default: pull from db).",
      "resource:\nName of the resource, eg: electric_demand.\nend_date:\nend date of the calculation, inclusive.\nAlong with above required parameters. Following are the optional parameters.\n\n\u2014floor_number:\nfloor for which run the algorithm.\n\n\u2014tz:\ntimezone of the building(default: pull from db).\nModules\ngenerate_peak\nThis module contains methods for pulling demand data, pulling the billing periods data, and use both of them to\ndetermine the max peak over the given billing period. if billing period is not found for thr current date, 1st day of the\nmonth will be used as the starting of the billing period.\nutils\nThis module has utility methods for following:\n\nConvert time to UTC timezone and ISO format.\n\nSave report to the output directory.\nNANTUM Common Modules\n69main\nThis module takes all of the above modules and builds the pipeline for fetching, cleaning data, and sorts the slices\nreturned by the logic module according to distance.\nFollowing these operations, the day selection is weighted by number of slices selected from a particular day.\nOnce the days are selected it fits a normal through the days and generate three time series with mean of the\ndistribution, upper bound and lower bound.\nInput to this module is the run parameters and output is a json with following fields.\n\ncompany\nName of the company.\n\nns\nNamespace of the building.\n\ndate\nDate of prediction.\n\nfloor\nFloor number.\n\noutput\nOutput is a dictionary of peak demand, timestamp at which the peak was found and period id.\nRequirement\n\nInput type\nDemand [consumption per unit of time]\n\nGranularity of data\nMinimum required granularity of data is 15-min. Higher granularity would NOT improve the accuracy of the\nalgorithm.\n\nSize\nMinimum size requirement before first implementqation is 1 day of data.\nExamples\nErrors and Solutions\nPredictive Demand\nIntroduction\nThis system takes into account outside temperature, relative humidity and current electric demand and predicts the\ndemand curve for rest of the day in real time.\nThe day is divided into 13 different slices.\nearly_morning, morning_transition, day_time1, day_time2\u2019, day_time3, day_time4, day_time5, day_time6,\nday_time7, day_time8, day_time9, evening_transition, night_time.\nThere is model to predict each slice and output of the previous slice model is input to the next model.",
      "There is model to predict each slice and output of the previous slice model is input to the next model.\nTo predict maximum values there are 10 different models. which predicts the maximum values of demand between 8\nAM to 16 PM on hourly basis.\nNANTUM Common Modules\n70System diagram\nUsability/ Limitations\nThis package can be used to predict high fidelity time series for any resource and is great for taking into account the\ndecisions made by the building operators as it reacts to the behaviour of the operator.\nLimitation:\nAt the beginning of the day, ideally output of this package will be same as that of the benchmarks. However, as the\nday progresses say, 10 AM the impact of current behavior will modify it and it will give more accurate prediction of\nthe day energy curve.\nInstallation and run\nDownload as zip or clone from the repository. Install dependencies from requirements.txt\nInstall the package with setup:\n$ git clone git@bitbucket.org:ALPD/predictive_demand.git\n$ python setup.py install\nuse run_predictive_demand with following parameters.\ncompany:\nName of the company\nbuilding:\nNamespace for the building.\nresource:\nName of the resource to be predicted, eg: electric_demand.\nAlong with above required parameters. Following are the optional parameters.\nNANTUM Common Modules\n71\n\u2014tz:\nTimezone of the building.\n\n\u2014floor_number:\nFloor number of the building, default is None(whole building).\n\n\u2014n_history_days:\nNumber of history days to use, (default is 360).\n\n\u2014lag:\nnumber of previous values to use in current point prediction, default 4.\n\n\u2014n_days_average:\nNumber of days to average on(use Average of these days demand to train demand for current day), default\nis 4.\n\n\u2014prediction_time:\nTime at which prediction is done. Default is current timestamp.\n\n\u2014algorithm:\nwhich algorithm to use for prediction, default is xgb regressor options are xgb, linear, RNN.\nModules\ndata_handler\nThis module contains methods for cleaning the data and preparing the rolling data (list of input as exterior\ntemperature, relative humidity and current demand as input and next demand point as output). This input output pairs\nlist is used by the training and prediction modules.\nconsumption_trainer\nThis module generates and trains the maximum_demand prediction models based upon the time of the day.",
      "This input output pairs\nlist is used by the training and prediction modules.\nconsumption_trainer\nThis module generates and trains the maximum_demand prediction models based upon the time of the day. This\nmodule also uses sliced benchmarks package to find the base or select days for training.\nIt takes in the company, building, timezone, start_date(training data start date), end_date(training data end date),\ngran_in (granularity of th data), current_day, selected_day(day selected based upon weather similarity) and returns\nmodels to predict maximum value of the demand. This module also synthesize data using normal distribution.\ntrain\nThis model trains the sliced demand curve models and returns trained slice models.\npredict\nThis model predicts the maximum, minimum and the demand curve for the given resource on given day and returns\npredictions.\nday_attributes\nThis module generates the day attributes for the given date. These day attributes are used by the savings calculator.\nOutput of this module is dictionary object with entries for overall demand of the day, overall consumption, on peak\ndemand, off peak demand, on peak consumption, on peak consumption, demand units and consumption units.\nmain\nThis module takes all of the above modules and builds the pipeline for fetching, cleaning data, training and predicting\ncurves.\nInput to this module is the run parameters and output is a json with following fields.\nNANTUM Common Modules\n72\ncompany\nName of the company.\n\nns\nNamespace of the building.\n\ndate\nDate of prediction.\n\nfloor\nFloor number.\n\noutput\nOutput is a dictionary with key value pairs as {time: value} Time is in ISO format in utc timezone.\nRequirements\n\nInput type\nDemand [consumption per unit of time] Weather forecast data\n\nGranularity of data\nMinimum required granularity of data is 15-min. Higher granularity would NOT improve the accuracy of the\nalgorithm.\n\nSize\nMinimum size requirement before first implementqation is 1 month of data. The variation of error (or\nuncertainty) with density of data is shown below.\nNANTUM Common Modules\n73Examples\nThe notebook for this package can be found here.\nErrors and Solutions\nOnboarding\nAnalytics Onboard Requirement\nOnboarding Playbook contains all the information on the requirement of data length and frequency to successfully\nrun each applications.",
      "NANTUM Common Modules\n73Examples\nThe notebook for this package can be found here.\nErrors and Solutions\nOnboarding\nAnalytics Onboard Requirement\nOnboarding Playbook contains all the information on the requirement of data length and frequency to successfully\nrun each applications. The link to the full document is here: Onboarding Playbook\nECM Input/Output Standard\nFor all the ECMs, the required input data and the expected output data are listed fully in the ECM Input/Output\nStandard document. The document also lists the type of the data for both input and output. In addition, ECM\ndatapoints matrix summarizes the data and info requirement for each ECM as well as their availability in NANTUM at\nthe current time.\nThe link to the full ECM Input/Output Standard document is here: ECM Input/Output Standard\nThe link to the full ECM Datapoints Matrix is here: ECM Datapoint Matrix\nOnboarding\n74Automated Recognition and Mapping (ARM)\nOverview\nModern commercial and residential buildings are equipped with thousands of sensing and control points. Labeling\nand tagging procedure of these points to be prepared for Nantum energy-comfort optimization algorithms (e.g.,\nNantum ECMs) is costly. In addition, these procedures require domain expertise with a good understanding of the\ntarget building context (systems, proprietary point naming schemes, etc.). Nantum\u2019s Automated Recognition and\nMapping is developed using advanced ML-based algorithms to significantly reduce the human effort required for\nmapping sensors to their context.\nAlgorithm\nUnsupervised Engine\n\nNaming Convention Clustering\n\nTime-series Signature Clustering\nSupervised Engine * Naming Convention Classification * Time-series Signature Classification\nImplementation and Results\nLimitations and Future Works\nNANTUM Marketplace\nAI of AIs (Nanum Decentralized AI Architecture)\nThe building system is a complex system-of-systems consisting of highly coupled systems including (i) HVAC, (ii)\nEnvironment, (iii) Occupancy, (iv) Building Characteristics, (v) Energy Storage, and (vi) Smart Gird. Each system\nalso consists of subsystems from a combination of sub-systems (e.g., HVAC is a combination of sub-systems\nincluding (i) ventilation, (ii) chiller, and (iii) condensing subsystems).",
      "Each system\nalso consists of subsystems from a combination of sub-systems (e.g., HVAC is a combination of sub-systems\nincluding (i) ventilation, (ii) chiller, and (iii) condensing subsystems).\nNantum Marketplace offers an advanced AI advisory system formulated based on deep Q-learning technology to\noptimize the whole-building level objectives while satisfying consistency requirements among the various system AIs\nby minimizing interdisciplinary discrepancies. This advanced advisory system will enable our clients access to a\nbroad array of 3rd party services, with the added flexibility of allowing them to bring their preferred vendors to the\nnetwork.\nNANTUM Marketplace\n75External Energy Conservation Measures (ECMs)\nBrain4Energy ECM\nPhase 1: Air-Side Optimization\nSummary\nThe overarching objective of the B4E energy optimization algorithm in the first phase is to control static pressure in\nthe AHU system to reduce AHU fan speed and therefore minimize horsepower (i.e., fan horsepower consumption for\na fixed system varies with the cube of fan speed).\nExternal Energy Conservation Measures (ECMs)\n76Loop Diagram\nExternal Energy Conservation Measures (ECMs)\n77Required Input\nPotential Output\n Output\n System\n 1\nSupply Air Temp. Setpoint\nAHU Air-Side Loop\nExternal Energy Conservation Measures (ECMs)\n782\nStatic Pressure Setpoint\nAHU Air-Side Loop\nImplementation Requirements\n\nData frequency : it is recommended to provide data in 5 min frequency.\n\nData Cleaning : B4E algorithms include the data cleaning engine [No need for Nantum Data Cleaning ].\n\nMinimum Required History (training data) : +2 Weeks\n\nRunning Algorithm Frequency : 5 minutes\nCase Study\n\n40 East 52nd St., New York: Local floor Direct Expansion (DX) Units\n\n55 Board St., New YorkL: Local floor Direct Expansion (DX) Units\n\n3 Times Square, New York: Central Electric Chiller, local chw AHU each floor, Fan powered vav boxes\n\n560 Lexington Avenue, New York: Central Electric Chiller, local chw AHU each floor, Fan powered vav boxes\nExternal Energy Conservation Measures (ECMs)\n79"
    ],
    "queries":"/future/u/okhattab/data/MSMARCO/queries.train.tsv",
    "index_name":"nantum_doc",
    "overwrite":false,
    "root":".ragatouille/",
    "experiment":"colbert",
    "index_root":null,
    "name":"2024-06/20/09.19.58",
    "rank":0,
    "nranks":1,
    "amp":true,
    "gpus":0,
    "avoid_fork_if_possible":false
  },
  "num_chunks":1,
  "num_partitions":1024,
  "num_embeddings":16325,
  "avg_doclen":398.1707317073,
  "RAGatouille":{
    "index_config":{
      "index_type":"PLAID",
      "index_name":"nantum_doc"
    }
  }
}